{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>The Digital Twin as a Service (DTaaS) software is useful to create and run digital twins. The digital twins that are running can be used as service by other users. These users need not be members of the DTaaS software platform itself.</p> <p>There is an overview of the software available for:</p> <ul> <li>General users - overview slides and overview video, feature walkthrough</li> <li>Developers - slides and video.</li> </ul> <p>There is also a research paper draft if you are interested in reading the scientific roadmap for this software.</p>"},{"location":"index.html#license","title":"License","text":"<p>This software is owned by The INTO-CPS Association and is available under the INTO-CPS License.</p> <p>The DTaaS software platform uses Tr\u00e6fik, ML Workspace, Grafana, InfluxDB and RabbitMQ open-source components. These software components have their own licenses.</p>"},{"location":"FAQ.html","title":"FAQ","text":""},{"location":"FAQ.html#abreviations","title":"Abreviations","text":"Term Full Form DT Digital Twin DTaaS Digital Twin as a Service PT Physical Twin"},{"location":"FAQ.html#general-questions","title":"General Questions","text":"What is DTaaS? <p>DTaaS is software platform on which you can create and run digital twins. Please see the features page to get a sense of the things you can do in DaaS.</p> Are there any Key Performance / Capability Indicators for DTaaS? Key Performance Indicator Value Processor Two AMD EPYC 7443 24-Core Processors Maximum Storage Capacity 4TB SSD, RAID 0 configuration Storage Type File System Maximum file size 10 GB Data transfer speed 100 Mbps Data Security Yes Data Privacy Yes Redundancy None Availability It is a matter of human resources. If you have human resources to maintain DTaaS round the clock, upwards 95% is easily possible. Do you provide licensed software like Matlab? <p>The licensed software are not available on the software platform. But users have private workspaces which are based on Linux-based xfce Desktop environment. Users can install software in their workspaces. The licensed software installed by one user is not available to another user.</p>"},{"location":"FAQ.html#digital-twin-models","title":"Digital Twin Models","text":"Can DTaaS create new DT models? <p>DTaaS is not a model creation  tool. You can put model creation tool inside DTaaS and create new models.</p> <p>The DTaaS itself does not create digital twin models. But you can run Linux desktop / terminal tools  inside the DTaaS. So you can create models inside DTaaS and run them using tools that can run in Linux. The Windows only tools can not run in DTaaS.</p> How can DTaaS help to design geometric model? Does it support 3D modeling and simulation? <p>Well, DTaaS by itself does not produce any models. DTaaS only provides a platform and an ecosystem of services to facilitate digital twins to be run as services. Since each user has a Linux OS at their disposal, they can also run digital twins that have graphical interface.</p> <p>In summary, DTaaS is neither a modeling nor simulation tool. If you need these kinds of tools, you need to bring them onto the platform. For example, if you need Matlab for your work, you need to bring the licensed Matlab software. </p> DTaas is not able to do any modelling or simulation in this case, like other platforms in market provide modelling and simulation alongside integration and UI. Is this a correct understanding? <p>Yes, you are right</p> Does it support XML-based representation and ontology representation? <p>Currently No. We are looking for users needing this capability. If you have concrete requirements and an example, we can discuss a way of realizing it in DTaaS. </p>"},{"location":"FAQ.html#communication-between-physical-twin-and-digital-twin","title":"Communication Between Physical Twin and Digital Twin","text":"How would you measure a physical entity like shape, size, weight, structure, chemical attributes etc. using DTaaS? Any specific technology used in this case? <p>The real measurements are done at physical twin which are then communicated to the digital twin. Any digital twin platform like DTaaS can only facilitate this communication of these measurements from physical twin. The DTaaS provides InfluxDB, RabbitMQ and Mosquitto services for this purpose. These three are probably most widely used services for digital twin communication. </p> <p>Having said that, DTaaS allows you to utilize other communication technologies and services hosted elsewhere on the Internet.</p> How a real-time data can be differed from static data and what is the procedure to identify dynamic data? Is there any UI or specific tool used here? <p>DTaaS can not understand the static or dynamic nature of data. It can facilitate storing names, units and any other text description of interesting quantities (weight of batter, voltage output etc). It can also store the data being sent by the physical twin. The distinction between static and dynamic data needs to be made by the user.</p> <p>Only metadata of the data can reveal such more information about the nature of data. A tool can probably help in very specific cases, but you need metadata. If there is a human being making this distinction, then the need for metadata goes down but does not completely go away.</p> <p>In some of the DT platforms supported by manufacturers, there is a tight integration between data and model. In this case, the tool itself is taking care of the metadata. The DTaaS is a generic platform which can support execution of digital twins. If a tool can be executed on a Linux desktop / commandline, the tool can be supported within DTaaS. The tool (ex. Matlab) itself can take care of the metadata requirements.</p> How can DTaaS control the physical entity? Which technologies it uses for controlling the physical world? <p>At a very abstract level, there is a communication from physical entity to digital entity and back to physical entity. How this communication should happen is decided by the person designing the digital entity. The DTaaS can provide communication services that can help you do this communication with relative ease. </p> <p>You can use InfluxDB, RabbitMQ and Mosquitto services hosted on DTaaS for two communication between digital and physical entities.</p>"},{"location":"FAQ.html#data-management","title":"Data Management","text":"Does DTaaS support data collection from different sources like hardware, software and network? Is there any user interface or any tracking instruments used for data collection? <p>The DTaaS provids InfluxDB, RabbitMQ, MQTT  services. Both the physical twin and digital twin can utilize these protocols for communication. The IoT (time-series) data can be collected using InfluxDB and MQTT broker services. There is a user interface for InfluxDB which can be used to analyze the data collected.</p> <p>Users can also manually upload their data files into DTaaS.</p> Which transmission protocol does DTaaS allow? <p>InfluxDB, RabbitMQ, MQTT and anything else that can be used from Cloud service providers.</p> Does DTaaS support multisource information and combined multi sensor input data? Can it provide analysis and decision-supporting inferences? <p>You can store information from multiple sources. The existing InfluxDB services hosted on DTaaS already has a dedicated Influx / Flux query language for doing sensor fusion, analysis and inferences.</p> Which kinds of visualization technologies DTaaS can support (e.g. graphical, geometry, image, VR/AR representation)? <p>Graphical, geometric and images. If you need specific licensed software for the visualization, you will have to bring the license for it. DTaaS does not support AR/VR.</p> Can DTaaS collect data directly from sensors? <p>Yes</p> Is DTaaS able to transmit data to cloud in real time? <p>Yes</p>"},{"location":"FAQ.html#platform-native-services-on-dtaas-platform","title":"Platform Native Services on DTaaS Platform","text":"Is DTaaS able to detect the anomalies about-to-fail components and prescribe solutions? <p>This is the job of a digital twin. If you have a ready to use digital twin that does the job, DTaaS allows others to use your solution.</p>"},{"location":"LICENSE.html","title":"LICENSE","text":"<p>--- Start of Definition of INTO-CPS Association Public License ---</p> <p>/  * This file is part of the INTO-CPS Association.  *  * Copyright (c) 2017-CurrentYear, INTO-CPS Association (ICA),  * c/o Peter Gorm Larsen, Aarhus University, Department of Engineering,  * Finlandsgade 22, 8200 Aarhus N, Denmark.  *  * All rights reserved.  *  * THIS PROGRAM IS PROVIDED UNDER THE TERMS OF GPL VERSION 3 LICENSE OR  * THIS INTO-CPS ASSOCIATION PUBLIC LICENSE (ICAPL) VERSION 1.0.  * ANY USE, REPRODUCTION OR DISTRIBUTION OF THIS PROGRAM CONSTITUTES  * RECIPIENT'S ACCEPTANCE OF THE INTO-CPS ASSOCIATION PUBLIC LICENSE OR   * THE GPL VERSION 3, ACCORDING TO RECIPIENTS CHOICE.  *  * The INTO-CPS tool suite software and the INTO-CPS Association   * Public License (ICAPL) are obtained from the INTO-CPS Association, either   * from the above address, from the URLs: http://www.into-cps.org or  * in the INTO-CPS tool suite distribution.  * GNU version 3 is obtained from: http://www.gnu.org/copyleft/gpl.html.  *  * This program is distributed WITHOUT ANY WARRANTY; without  * even the implied warranty of  MERCHANTABILITY or FITNESS  * FOR A PARTICULAR PURPOSE, EXCEPT AS EXPRESSLY SET FORTH  * IN THE BY RECIPIENT SELECTED SUBSIDIARY LICENSE CONDITIONS OF  * THE INTO-CPS ASSOCIATION PUBLIC LICENSE.  *  * See the full ICAPL conditions for more details.  *  /</p> <p>--- End of INTO-CPS Association Public License Header ---</p> <p>The ICAPL is a public license for the INTO-CPS tool suite with three modes/alternatives (GPL, ICA-Internal-EPL, ICA-External-EPL) for use and redistribution, in source and/or binary/object-code form:</p> <ul> <li> <p>GPL. Any party (member or non-member of the INTO-CPS Association) may use and    redistribute INTO-CPS tool suite under GPL version 3.</p> </li> <li> <p>Silver Level members of the INTO-CPS Association may also use and redistribute    the INTO-CPS tool suite under ICA-Internal-EPL conditions.</p> </li> <li> <p>Gold Level members of the INTO-CPS Association may also use and redistribute    The INTO-CPS tool suite under ICA-Internal-EPL or ICA-External-EPL conditions.</p> </li> </ul> <p>Definitions of the INTO-CPS Association Public license modes:</p> <ul> <li> <p>GPL = GPL version 3.</p> </li> <li> <p>ICA-Internal-EPL = These INTO-CPA Association Public license conditions together with  Internally restricted EPL, i.e., EPL version 1.0 with the Additional Condition   that use and redistribution by a member of the INTO-CPS Association is only allowed  within the INTO-CPS Association member's own organization (i.e., its own legal entity),  or for a member of the INTO-CPS Association paying a membership fee corresponding to   the size of the organization including all its affiliates, use and redistribution  is allowed within/between its affiliates.</p> </li> <li> <p>ICA-External-EPL = These INTO-CPA Association Public license conditions together with  Externally restricted EPL, i.e., EPL version 1.0 with the Additional Condition   that use and redistribution by a member of the INTO-CPS Association, or by a Licensed  Third Party Distributor having a redistribution agreement with that member,  to parties external to the INTO-CPS Association member\u2019s own organization (i.e., its own  legal entity) is only allowed in binary/object-code form, except the case of  redistribution to other members the INTO-CPS Association to which source is also   allowed to be distributed.</p> </li> </ul> <p>[This has the consequence that an external party who wishes to use the INTO-CPS Association in source form together with its own proprietary software in all cases must be a member of the INTO-CPS Association].</p> <p>In all cases of usage and redistribution by recipients, the following conditions also apply:</p> <p>a) Redistributions of source code must retain the above copyright notice,  all definitions, and conditions. It is sufficient if the ICAPL Header is  present in each source file, if the full ICAPL is available in a prominent  and easily located place in the redistribution.</p> <p>b) Redistributions in binary/object-code form must reproduce the above  copyright notice, all definitions, and conditions. It is sufficient if the  ICAPL Header and the location in the redistribution of the full ICAPL  are present in the documentation and/or other materials provided with the  redistribution, if the full ICAPL is available in a prominent and easily  located place in the redistribution.</p> <p>c) A recipient must clearly indicate its chosen usage mode of ICAPL,  in accompanying documentation and in a text file ICA-USAGE-MODE.txt,  provided with the distribution.</p> <p>d) Contributor(s) making a Contribution to the INTO-CPS Association thereby also makes a  Transfer of Contribution Copyright. In return, upon the effective date of  the transfer, ICA grants the Contributor(s) a Contribution License of the  Contribution. ICA has the right to accept or refuse Contributions.</p> <p>Definitions:</p> <p>\"Subsidiary license conditions\" means:</p> <p>The additional license conditions depending on the by the recipient chosen mode of ICAPL, defined by GPL version 3.0 for GPL, and by EPL for ICA-Internal-EPL and ICA-External-EPL.</p> <p>\"ICAPL\" means:</p> <p>INTO-CPS Association Public License version 1.0, i.e., the license defined here (the text between \"--- Start of Definition of INTO-CPS Association Public License ---\" and \"--- End of Definition of INTO-CPS Association Public License ---\", or later versions thereof.</p> <p>\"ICAPL Header\" means:</p> <p>INTO-CPS Association Public License Header version 1.2, i.e., the text between \"--- Start of Definition of INTO-CPS Association Public License ---\" and \"--- End of INTO-CPS Association Public License Header ---, or later versions thereof.</p> <p>\"Contribution\" means:</p> <p>a) in the case of the initial Contributor, the initial code and documentation  distributed under ICAPL, and</p> <p>b) in the case of each subsequent Contributor:    i) changes to the INTO-CPS tool suite, and   ii) additions to the INTO-CPS tool suite;</p> <p>where such changes and/or additions to the INTO-CPS tool suite originate from and are distributed by that particular Contributor. A Contribution 'originates' from a Contributor if it was added to the INTO-CPS tool suite by such Contributor itself or anyone acting on such Contributor's behalf.</p> <p>For Contributors licensing the INTO-CPS tool suite under ICA-Internal-EPL or ICA-External-EPL conditions, the following conditions also hold:</p> <p>Contributions do not include additions to the distributed Program which: (i) are separate modules of software distributed in conjunction with the INTO-CPS tool suite under their own license agreement, (ii) are separate modules which are not derivative works of the INTO-CPS tool suite, and (iii) are separate modules of software distributed in conjunction with the INTO-CPS tool suite under their own license agreement where these separate modules are merged with (weaved together with) modules of The INTO-CPS tool suite to form new modules that are distributed as object code or source code under their own license agreement, as allowed under the Additional Condition of internal distribution according to ICA-Internal-EPL and/or Additional Condition for external distribution according to ICA-External-EPL.</p> <p>\"Transfer of Contribution Copyright\" means that the Contributors of a Contribution transfer the ownership and the copyright of the Contribution to the INTO-CPS Association, the INTO-CPS Association Copyright owner, for inclusion in the INTO-CPS tool suite. The transfer takes place upon the effective date when the Contribution is made available on the INTO-CPS Association web site under ICAPL, by such Contributors themselves or anyone acting on such Contributors' behalf. The transfer is free of charge. If the Contributors or the INTO-CPS Association so wish, an optional Copyright transfer agreement can be signed between the INTO-CPS Association and the Contributors.</p> <p>\"Contribution License\" means a license from the INTO-CPS Association to the Contributors of the Contribution, effective on the date of the Transfer of Contribution Copyright, where the INTO-CPS Association grants the Contributors a non-exclusive, world-wide, transferable, free of charge, perpetual license, including sublicensing rights, to use, have used, modify, have modified, reproduce and or have reproduced the contributed material, for business and other purposes, including but not limited to evaluation, development, testing, integration and merging with other software and distribution. The warranty and liability disclaimers of ICAPL apply to this license.</p> <p>\"Contributor\" means any person or entity that distributes (part of) the INTO-CPS tool chain.</p> <p>\"The Program\" means the Contributions distributed in accordance with ICAPL.</p> <p>\"The INTO-CPS tool chain\" means the Contributions distributed in accordance with ICAPL.</p> <p>\"Recipient\" means anyone who receives the INTO-CPS tool chain under ICAPL, including all Contributors.</p> <p>\"Licensed Third Party Distributor\" means a reseller/distributor having signed a redistribution/resale agreement in accordance with ICAPL and the INTO-CPS  Association Bylaws, with a Gold Level organizational member which is not an  Affiliate of the reseller/distributor, for distributing a product containing  part(s) of the INTO-CPS tool suite. The Licensed Third Party Distributor shall  only be allowed further redistribution to other resellers if the Gold Level  member is granting such a right to it in the redistribution/resale agreement  between the Gold Level member and the Licensed Third Party Distributor.</p> <p>\"Affiliate\" shall mean any legal entity, directly or indirectly, through one or more intermediaries, controlling or controlled by or under common control with any other legal entity, as the case may be. For purposes of this definition, the term \"control\" (including the terms \"controlling,\" \"controlled by\" and \"under common control with\") means the possession, direct or indirect, of the power to direct or cause the direction of the management and policies of a legal entity, whether through the ownership of voting securities, by contract or otherwise.</p> <p>NO WARRANTY</p> <p>EXCEPT AS EXPRESSLY SET FORTH IN THE BY RECIPIENT SELECTED SUBSIDIARY LICENSE CONDITIONS OF ICAPL, THE INTO-CPS ASSOCIATION IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the INTO-CPS tool suite and assumes all risks associated with its exercise of rights under ICAPL , including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations.</p> <p>DISCLAIMER OF LIABILITY</p> <p>EXCEPT AS EXPRESSLY SET FORTH IN THE BY RECIPIENT SELECTED SUBSIDIARY LICENSE CONDITIONS OF ICAPL, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE INTO-CPS TOOL  SUITE OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED  OF THE POSSIBILITY OF SUCH DAMAGES.</p> <p>A Contributor licensing the INTO-CPS tool suite under ICA-Internal-EPL or ICA-External-EPL may choose to distribute (parts of) the INTO-CPS tool suite  in object code form under its own license agreement, provided that:</p> <p>a) it complies with the terms and conditions of ICAPL; or for the case of redistribution of the INTO-CPS tool suite together with proprietary code it is a dual license where the INTO-CPS tool suite parts are distributed under ICAPL compatible conditions and the proprietary code is distributed under proprietary license conditions; and</p> <p>b) its license agreement:    i) effectively disclaims on behalf of all Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose;   ii) effectively excludes on behalf of all Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits;  iii) states that any provisions which differ from ICAPL are offered by that Contributor alone and not by any other party; and   iv) states from where the source code for the INTO-CPS tool suite is available, and informs licensees how to obtain it in a reasonable manner on or through a medium customarily used for software exchange.</p> <p>When the INTO-CPS tool suite is made available in source code form:</p> <p>a) it must be made available under ICAPL; and</p> <p>b) a copy of ICAPL must be included with each copy of the INTO-CPS tool suite.</p> <p>c) a copy of the subsidiary license associated with the selected mode of ICAPL must be included with each copy of the INTO-CPS tool suite.</p> <p>Contributors may not remove or alter any copyright notices contained within The INTO-CPS tool suite.</p> <p>If there is a conflict between ICAPL and the subsidiary license conditions, ICAPL has priority.</p> <p>This Agreement is governed by the laws of Denmark. The place of jurisdiction for all disagreements related to this Agreement, is Aarhus, Denmark.</p> <p>The EPL 1.0 license definition has been obtained from: http://www.eclipse.org/legal/epl-v10.html. It is also reproduced in the INTO-CPS distribution.</p> <p>The GPL Version 3 license definition has been obtained from http://www.gnu.org/copyleft/gpl.html. It is also reproduced in the INTO-CPS distribution.</p> <p>--- End of Definition of INTO-CPS Association Public License ---</p>"},{"location":"PUBLISH.html","title":"Project Documentation","text":"<p>This file contains instructions for creation, compilation and publication of project documentation.</p> <p>The documentation system is based on Material for Mkdocs. The documentation is generated based on the configuration files:</p> <ul> <li>mkdocs.yml: used for generating online documentation which is hosted on the web</li> <li>mkdocs_offline.yml: used for generating offline documentation that can be downloaded and used on user computer.</li> </ul> <p>Install Mkdocs using the following command.</p> <pre><code>pip install -r docs/requirements.txt\n</code></pre>"},{"location":"PUBLISH.html#create-documentation","title":"Create documentation","text":"<p>You can add, and edit the markdown files in <code>docs/</code> directory to update the documentation. There is a facility to check the status of your documentation by using:</p> <pre><code>mkdocs serve --config-file mkdocs.yml\n</code></pre>"},{"location":"PUBLISH.html#publish-documentation","title":"Publish documentation","text":"<p>You can compile and place the html version of documentation on the <code>webpage-docs</code> branch of the codebase.</p> <pre><code>source script/docs.sh [version]\n</code></pre> <p>The command takes an optional version parameter. This version parameter is needed for making a release. Otherwise, the documentation gets published with the latest version tag. This command makes a new commit on <code>webpage-docs</code> branch. You need to push the branch to upstream.</p> <pre><code>git push webpage-docs\n</code></pre> <p>The github pages system serves the project documentation from this branch.</p>"},{"location":"bugs.html","title":"Bugs","text":""},{"location":"bugs.html#some-limitations","title":"Some limitations","text":"<ul> <li>The complete DTaaS software requires multiple docker containers and one client website. All of these can work together only on a server with a proper domain name. The complete application does not work on localhost.</li> </ul>"},{"location":"bugs.html#third-party-software","title":"Third-Party Software","text":"<p>We use third-party software which have certain known issues. Some of the issues are listed below.</p>"},{"location":"bugs.html#ml-workspace","title":"ML Workspace","text":"<ul> <li>the docker container loses network connectivity after three days. The only known solution is to restart the docker container. You don't need to restart the complete DTaaS platform, restart of the docker contaienr of ml-workspace is sufficient.</li> <li>the terminal tool doesn't seem to have the ability to refresh itself. If there is an issue, the only solution is to close and reopen the terminal from \"open tools\" drop down of notebook</li> <li>terminal app does not show at all after some time: terminal always comes if it is open from drop-down menu of Jupyter Notebook, but not as a direct link.</li> </ul>"},{"location":"bugs.html#gitlab","title":"Gitlab","text":"<ul> <li>The gilab oauth authentication service does not have a way to sign out of a third-party application. Even if you sign out of DTaaS, the gitlab still shows user as signed in. The next time you click on the sign in button on the DTaaS page, user is not shown the login page. Instead user is directly taken to the Library page. So close the brower window after you are done. Another way to overcome this limitation is to open your gitlab instance (<code>https://gitlab.foo.com</code>) and signout from there. Thus user needs to sign out of two places, namely DTaaS and gitlab, in order to completely exit the DTaaS application.</li> </ul>"},{"location":"thanks.html","title":"Thanks","text":""},{"location":"thanks.html#contributors","title":"Contributors","text":"<p>code contributors</p>"},{"location":"thanks.html#users","title":"Users","text":"<p>Cl\u00e1udio \u00c2ngelo Gon\u00e7alves Gomes, Dmitri Tcherniak, Elif Ecem Bas, Giuseppe Abbiati, Hao Feng, Henrik Ejersbo, Tanusree Roy</p>"},{"location":"thanks.html#documentation","title":"Documentation","text":"<ol> <li>Talasila, P., Gomes, C., Mikkelsen, P. H., Arboleda, S. G., Kamburjan, E., &amp; Larsen, P. G. (2023). Digital Twin as a Service (DTaaS): A Platform for Digital Twin Developers and Users. arXiv preprint arXiv:2305.07244.</li> </ol>"},{"location":"admin/host.html","title":"DTaaS on Linux Operating System","text":"<p>These are installation instructions for running DTaaS application on a Ubuntu Server 20.04 Operating System. The setup requires a machine which can spare 16GB RAM, 8 vCPUs and 50GB Hard Disk space.</p> <p>A dummy foo.com URL has been used for illustration. Please change this to your unique website URL. It is assumed that you are going to serve the application in only HTTPS mode.</p> <p>Please follow these steps to make this work in your local environment. Download the DTaaS.zip from the releases page. Unzip the same into a directory named DTaaS. The rest of the instructions assume that your working directory is DTaaS.</p> <p>Note</p> <p>If you only want to test the application and are not setting up a production instance, you can follow the instructions of trial installation.</p>"},{"location":"admin/host.html#configuration","title":"Configuration","text":"<p>You need to configure the Traefik gateway, library microservice and react client website.</p> <p>The first step is to decide on the number of users and their usenames. The traefik gateway configuration has a template for two users. You can modify the usernames in the template to the usernames chosen by you.</p>"},{"location":"admin/host.html#the-traefik-gateway-server","title":"The traefik gateway server","text":"<p>You can run the Run the Traefik gateway server in both and HTTPS HTTPS mode to experience the DTaaS application. The installation guide assumes that you can run the application in HTTPS mode.</p> <p>The Traefik gateway configuration is at deploy/config/gateway/fileConfig.yml. Change <code>foo.com</code> to your local hostname and user1/user2 to the usernames chosen by you.</p> <p>Tip</p> <p>Do not use <code>http://</code> or <code>https://</code> in deploy/config/gateway/fileConfig.yml.</p>"},{"location":"admin/host.html#authentication","title":"Authentication","text":"<p>The dummy username is <code>foo</code> and the password is <code>bar</code>. Please change this before starting the gateway.</p> <pre><code>rm deploy/config/gateway/auth\ntouch deploy/config/gateway/auth\nhtpasswd deploy/config/gateway/auth &lt;first_username&gt;\npassword: &lt;your password&gt;\n</code></pre> <p>The user credentials added in deploy/config/gateway/auth should match the usernames in deploy/config/gateway/fileConfig.yml.</p>"},{"location":"admin/host.html#configure-lib-microservice","title":"Configure lib microservice","text":"<p>The first step in this configuration is to prepare the a filesystem for users. An example file system in <code>files/</code> directory. You can rename the top-level user1/user2 to the usernames chosen by you.</p> <p>Update the deploy/config/lib of the library microservice. The simplest possibility is to use <code>local</code> mode with the following example. The filepath is the absolute filepath to <code>files/</code> directory.</p> <pre><code>PORT='4001'\nMODE='local'\nLOCAL_PATH ='filepath'\nLOG_LEVEL='debug'\nAPOLLO_PATH='/lib'\nGRAPHQL_PLAYGROUND='true'\n</code></pre>"},{"location":"admin/host.html#configure-react-website","title":"Configure react website","text":"<p>Change the React website configuration in deploy/config/client/env.js.</p> <pre><code>window.env = {\nREACT_APP_ENVIRONMENT: 'prod',\nREACT_APP_URL: 'https://foo.com/',\nREACT_APP_URL_BASENAME: '',\nREACT_APP_URL_DTLINK: '/lab',\nREACT_APP_URL_LIBLINK: '',\nREACT_APP_WORKBENCHLINK_TERMINAL: '/terminals/main',\nREACT_APP_WORKBENCHLINK_VNCDESKTOP: '/tools/vnc/?password=vncpassword',\nREACT_APP_WORKBENCHLINK_VSCODE: '/tools/vscode/',\nREACT_APP_WORKBENCHLINK_JUPYTERLAB: '/lab',\nREACT_APP_WORKBENCHLINK_JUPYTERNOTEBOOK: '',\n};\n</code></pre>"},{"location":"admin/host.html#update-the-installation-script","title":"Update the installation script","text":"<p>Open <code>deploy/install.sh</code> and update user1/user2 to usernames chosen by you.</p>"},{"location":"admin/host.html#perform-the-installation","title":"Perform the Installation","text":"<p>Go to the DTaaS directory and execute</p> <pre><code>source deploy/install.sh\n</code></pre> <p>You can run this script multiple times until the installation is successful.</p>"},{"location":"admin/host.html#access-the-application","title":"Access the application","text":"<p>Now you should be able to access the DTaaS application at: https://foo.com</p>"},{"location":"admin/trial.html","title":"Trial Installation","text":"<p>A single step install script is helpful in performing a trial run of the software. This script installs DTaaS software with default credentials and users on a Ubuntu server OS. You can use it to check a test installation of DTaaS.</p> <pre><code>wget https://github.com/INTO-CPS-Association/DTaaS/releases/download/v0.2.0/single-script-install.sh\nbash single-script-install.sh\n</code></pre> <p>Warning</p> <p>This test installation has default credentials and is thus highly insecure.</p>"},{"location":"admin/client/CLIENT.html","title":"Host the DTaaS Client Website","text":"<p>To host DTaaS client website on your server, follow these steps:</p> <ul> <li>Download the DTaaS-client.zip from the releases page.</li> <li> <p>Inside the <code>DTaaS-client</code> directory, there is <code>site</code> directory. The <code>site</code> directory contains all the optimized static files that are ready for deployment.</p> </li> <li> <p>Locate the file <code>site/env.js</code> and replace the example values to match your infrastructure. See the definitions below:</p> <pre><code>window.env = {\nREACT_APP_ENVIRONMENT: \"prod | dev\",\nREACT_APP_URL: \"URL for the gateway\",\nREACT_APP_URL_BASENAME: \"Base URL for the client website\"(optional),\nREACT_APP_URL_DTLINK: \"Endpoint for the Digital Twin\",\nREACT_APP_URL_LIBLINK: \"Endpoint for the Library Assets\",\nREACT_APP_WORKBENCHLINK_TERMINAL: \"Endpoint for the terminal link\",\nREACT_APP_WORKBENCHLINK_VNCDESKTOP: \"Endpoint for the VNC Desktop link\",\nREACT_APP_WORKBENCHLINK_VSCODE: \"Endpoint for the VS Code link\",\nREACT_APP_WORKBENCHLINK_JUPYTERLAB: \"Endpoint for the Jupyter Lab link\",\nREACT_APP_WORKBENCHLINK_JUPYTERNOTEBOOK:\n\"Endpoint for the Jupyter Notebook link\",\n};\n// Example values with no base URL. Trailing and ending slashes are optional.\nwindow.env = {\nREACT_APP_ENVIRONMENT: 'prod',\nREACT_APP_URL: 'https://foo.com/',\nREACT_APP_URL_BASENAME: '',\nREACT_APP_URL_DTLINK: '/lab',\nREACT_APP_URL_LIBLINK: '',\nREACT_APP_WORKBENCHLINK_TERMINAL: '/terminals/main',\nREACT_APP_WORKBENCHLINK_VNCDESKTOP: '/tools/vnc/?password=vncpassword',\nREACT_APP_WORKBENCHLINK_VSCODE: '/tools/vscode/',\nREACT_APP_WORKBENCHLINK_JUPYTERLAB: '/lab',\nREACT_APP_WORKBENCHLINK_JUPYTERNOTEBOOK: '',\n};\n</code></pre> </li> <li> <p>Copy the entire contents of the build folder to the root directory of your server where you want to deploy the app. You can use FTP, SFTP, or any other file transfer protocol to transfer the files.</p> </li> <li> <p>Make sure your server is configured to serve static files. This can vary depending on the server technology you are using, but typically you will need to configure your server to serve files from a specific directory.</p> </li> <li> <p>Once the files are on your server, you should be able to access your app by visiting your server's IP address or domain name in a web browser.</p> </li> </ul> <p> The website depends on Traefik gateway and ML Workspace components to be available. Otherwise, you only get a skeleton non-functional website.</p>"},{"location":"admin/client/CLIENT.html#complementary-components","title":"Complementary Components","text":"<p>The website requires background services for providing actual functionality. The minimum background service required is atleast one ML Workspace serving the following routes.</p> <pre><code>https:foo.com/&lt;username&gt;/lab\nhttps:foo.com/&lt;username&gt;/terminals/main'\nhttps:foo.com/&lt;username&gt;/tools/vnc/?password=vncpassword\nhttps:foo.com/&lt;username&gt;/tools/vscode/\n</code></pre> <p>The <code>username</code> is the user workspace created using ML Workspace docker container. Please follow the instructions in README. You can create as many user workspaces as you want. If you have two users - alice and bob - on your system, then the following the commands in  will instantiate the required user workspaces.</p> <pre><code>mkdir -p files/alice files/bob files/common\n\nprintf \"\\n\\n start the user workspaces\"\ndocker run -d \\\n-p 8090:8080 \\\n--name \"ml-workspace-alice\" \\\n-v \"$(pwd)/files/alice:/workspace\" \\\n-v \"$(pwd)/files/common:/workspace/common\" \\\n--env AUTHENTICATE_VIA_JUPYTER=\"\" \\\n--env WORKSPACE_BASE_URL=\"alice\" \\\n--shm-size 512m \\\n--restart always \\\nmltooling/ml-workspace:0.13.2\n\ndocker run -d \\\n-p 8091:8080 \\\n--name \"ml-workspace-bob\" \\\n-v \"$(pwd)/files/bob:/workspace\" \\\n-v \"$(pwd)/files/common:/workspace/common\" \\\n--env AUTHENTICATE_VIA_JUPYTER=\"\" \\\n--env WORKSPACE_BASE_URL=\"bob\" \\\n--shm-size 512m \\\n--restart always \\\nmltooling/ml-workspace:0.13.2\n</code></pre> <p>Given that multiple services are running at different routes, a reverse proxy is needed to map the background services to external routes. You can use Apache, NGINX, Traefik or any other software to work as reverse proxy.</p>"},{"location":"admin/servers/lib/LIB-MS.html","title":"Host Library Microservice","text":"<p>The lib microservice is a simplified file manager providing graphQL API. It has three features:</p> <ul> <li>provide a listing of directory contents.</li> <li>transfer a file to user.</li> <li>Source files can either come from local file system or from a gitlab instance.</li> </ul> <p>The library microservice is designed to manage and serve files, functions, and models to users, allowing them to access and interact with various resources.</p> <p>This document provides instructions for running a stand alone library microservice.</p>"},{"location":"admin/servers/lib/LIB-MS.html#setup-the-file-system","title":"Setup the File System","text":"<p>The users expect the following file system structure for their reusable assets.</p> <p></p> <p>There is a skeleton file structure in DTaaS codebase. You can copy and create file system for your users.</p>"},{"location":"admin/servers/lib/LIB-MS.html#gitlab-setup-optional","title":"Gitlab setup (optional)","text":"<p>For this microserivce to be functional, a certain directory or gitlab project structure is expected. The microservice expects that the gitlab consisting of one group, DTaaS, and within that group, all of the projects be located, user1, user2, ... , as well as a commons project. Each project corresponds to files of one user. A sample file structure can be seen in gitlab dtaas group. You can visit the gitlab documentation on groups for help on the management of gitlab groups.</p> <p>You can clone the git repositories from the <code>dtaas</code> group to get a sample file system structure for the lib microservice.</p>"},{"location":"admin/servers/lib/LIB-MS.html#setup-microservice","title":"Setup Microservice","text":"<p>To set up the lib microservice, follow these steps:</p> <p>Download the lib-microservice.zip from the releases page.</p>"},{"location":"admin/servers/lib/LIB-MS.html#configuration-setup","title":"Configuration setup","text":"<p>The microservices uses <code>.env</code> environment files to receive configuration.</p> <p>To set up the environment variables for the lib microservice, create a new file named .env in the <code>lib-ms</code> directory. Then, add the following variables and their respective values. Below you can see and how, with included examples:</p> <pre><code>PORT='4001'\nMODE='local' or 'gitlab'\nLOCAL_PATH ='/Users/&lt;Username&gt;/DTaaS/files'\nGITLAB_GROUP ='dtaas'\nGITLAB_URL='https://gitlab.com/api/graphql'\nTOKEN='123-sample-token'\nLOG_LEVEL='debug'\nTEST_PATH='/Users/&lt;Username&gt;/DTaaS/servers/lib/test/data/test_assets'\nAPOLLO_PATH='/lib' or ''\nGRAPHQL_PLAYGROUND='false' or 'true'\n</code></pre> <p>The <code>LOCAL_PATH</code> variable is the absolute filepath to the location of the local directory which will be served to users by the Library microservice.</p> <p>The <code>GITLAB_URL</code>, <code>GITLAB_GROUP</code> and <code>TOKEN</code> are only relevant for <code>gitlab</code> mode. The <code>TOKEN</code> should be set to your GitLab Group access API token. For more information on how to create and use your access token, gitlab page.</p> <p>Once you've generated a token, copy it and replace the value of <code>TOKEN</code> with your token for the gitlab group, can be found.</p> <p>Replace the default values the appropriate values for your setup.</p> <p>NOTE:</p> <ol> <li>When _MODE=local, only LOCAL_PATH is used. Other environment variables are unused.</li> <li>When MODE=gitlab, GITLAB_URL, TOKEN, and GITLAB_GROUP are used; LOCAL_PATH is unused.</li> </ol>"},{"location":"admin/servers/lib/LIB-MS.html#start-microservice","title":"Start Microservice","text":"<pre><code>yarn install    # Install dependencies for the microservice\nyarn build      # build the application\nyarn start      # start the application\n</code></pre> <p>You can press <code>Ctl+C</code> to halt the application. If you wish to run the microservice in the background, use</p> <pre><code>nohup yarn start &amp; disown\n</code></pre> <p>The lib microservice is now running and ready to serve files, functions, and models.</p> <p>Users can access the library microservice at URL: <code>http://localhost:&lt;PORT&gt;/lib</code>.</p>"},{"location":"admin/servers/lib/LIB-MS.html#developer-commands","title":"Developer Commands","text":"<pre><code>yarn install    # Install dependencies for the microservice\nyarn syntax     # analyzes source code for potential errors, style violations, and other issues,\nyarn build      # compile ES6 files into ES5 javascript files and copy all JS files into build/ directory\nyarn test -a      # run all tests\nyarn test -e      # run end-to-end tests\nyarn test -i      # run integration tests\nyarn test -u      # run unit tests\nyarn start      # start the application\nyarn clean      # deletes directories \"build\", \"coverage\", and \"dist\"\n</code></pre>"},{"location":"admin/servers/lib/LIB-MS.html#service-endpoint","title":"Service Endpoint","text":"<p>The URL endpoint for this microservice is located at: <code>localhost:PORT/lib</code></p>"},{"location":"admin/vagrant/base-box.html","title":"Vagrant Box for DTaaS","text":"<p>There are some good vagrant boxes on vagrant website. But these boxes require too many installations that take a long time and network bandwidth. So it is efficient to create one local vagrant box for DTaaS application and reuse the same in all installations.</p>"},{"location":"admin/vagrant/base-box.html#installed-software","title":"Installed Software","text":"<p>This base DTaaS vagrant box, when it is successfully created, has the following software:</p> <ul> <li>docker</li> <li>nodejs and yarn</li> <li>jupyter</li> <li>microk8s</li> <li>containers</li> <li>mltooling/ml-workspace:0.13.2</li> <li>traefik2.5</li> <li>influxdb2.4</li> <li>grafana</li> <li>telegraf</li> <li>gitlab</li> </ul>"},{"location":"admin/vagrant/base-box.html#create-the-vagrant-box","title":"Create the vagrant box","text":"<p>Publish a base virtualbox package to be used by vagrant to publish all other virtualbox packages</p> <pre><code>#create a key pair\nssh-keygen -b 4096 -t rsa -f key -q -N \"\"\nmv key vagrant\nmv key.pub vagrant.pub\n\nvagrant up\n\n# let the provisioning be complete\n# replace the vagrant ssh key-pair with personal one\nvagrant ssh\n\n# install the oh-my-zsh\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n# install plugins: history, autosuggestions, \ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\n\n# inside ~/.zshrc, modify the following line\nplugins=(git zsh-autosuggestions history cp tmux)\n# remove the vagrant default public key - first line of \n# /home/vagrant/.ssh/authorized_keys\n# exit vagrant guest machine and then\n# copy own private key to vagrant private key location\ncp vagrant .vagrant/machines/default/virtualbox/private_key\n\n# check\nvagrant ssh #should work\nvagrant halt\n\nvagrant package --base dtaas \\\n--info \"info.json\" --output dtaas.vagrant\n\n# Add box to the vagrant cache in ~/.vagrant.d/boxes directory\nvagrant box add --name dtaas ./dtaas.vagrant\n\n# You can use this box in other vagrant boxes using\n#config.vm.box = \"dtaas\"\n</code></pre>"},{"location":"admin/vagrant/single-machine.html","title":"DTaaS on Single Vagrant Machine","text":"<p>These are installation instructions for running DTaaS application inside one vagrant Virtual Machine.  The setup requires a machine which can spare 16GB RAM, 8 vCPUs and 50GB Hard Disk space to the vagrant box.</p> <p>A dummy foo.com URL has been used for illustration. Please change this to your unique website URL.</p> <p>Please follow these steps to make this work in your local environment.</p> <ol> <li>Create dtaas Vagrant box. Copy vagrant SSH private key into deploy/vagrant/single-machine. This shall be useful for logging into the vagrant machine created for single-machine deployment. You would have created an SSH key pair - vagrant and vagrant.pub. The vagrant is the private SSH key and is needed for the next steps.</li> <li>Update the Vagrantfile. Fields to update are:<ol> <li>Hostname (<code>node.vm.hostname = \"foo.com\"</code>)</li> <li>MAC address (<code>:mac =&gt; \"xxxxxxxx\"</code>). This change is required if you have a DHCP server assigning domain names based on MAC address. Otherwise, you can leave this field unchanged.</li> <li>Other adjustments are optional.</li> </ol> </li> <li>Execute the following commands from terminal</li> </ol> <pre><code>vagrant up\nvagrant ssh\n</code></pre> <p>Set a cronjob inside the vagrant virtual machine to remote the conflicting default route.</p> <pre><code>wget https://raw.githubusercontent.com/INTO-CPS-Association/DTaaS/release-v0.2/deploy/vagrant/route.sh\nsudo bash route.sh\n</code></pre> <p>If you only want to test the application and are not setting up a production instance, you can follow the instructions of single script install.</p> <p>If you are not in a hurry and would rather have a production instance, follow the instructions of regular server installation setup to complete the installation.</p>"},{"location":"admin/vagrant/two-machines.html","title":"DTaaS on Two Vagrant Machines","text":"<p>These are installation instructions for running DTaaS application in two vagrant virtual machines (VMs). In this setup, all the user workspaces shall be run on server1 while all the platform services will be run on server2.</p> <p>The setup requires two server VMs with the following hardware configuration:</p> <p>server1: 16GB RAM, 8 x64 vCPUs and 50GB Hard Disk space</p> <p>server2: 6GB RAM, 3 x64 vCPUs and 50GB Hard Disk space</p> <p>Under the default configuration, two user workspaces are provisioned on server1. The default installation setup also installs InfluxDB, Grafana and RabbitMQ services on server2. If you would like to install more services, you can create shell scripts to install the same on server2.</p>"},{"location":"admin/vagrant/two-machines.html#create-base-vagrant-box","title":"Create Base Vagrant Box","text":"<p>Create dtaas Vagrant box. You would have created an SSH key pair - vagrant and vagrant.pub. The vagrant is the private SSH key and is needed for the next steps. Copy vagrant SSH private key into the current directory (<code>deploy/vagrant/single-machine</code>). This shall be useful for logging into the vagrant machines created for two-machine deployment.</p>"},{"location":"admin/vagrant/two-machines.html#configure-server-settings","title":"Configure Server Settings","text":"<p>NOTE: A dummy foo.com and services.foo.com  URLs has been used for illustration. Please change these to your unique website URLs.</p> <p>The first step is to define the network identity of the two VMs. For that, you need server name, hostname and MAC address. The hostname is the network URL at which the server can be accessed on the web. Please follow these steps to make this work in your local environment.</p> <p>Update the boxes.json. There are entries one for each server. The fields to update are:</p> <ol> <li><code>name</code> - name of server1 (<code>\"name\" = \"dtaas\"</code>)</li> <li><code>hostname</code> - hostname of server1 (<code>\"name\" = \"foo.com\"</code>)</li> <li>MAC address (<code>:mac =&gt; \"xxxxxxxx\"</code>). This change is required if you have a DHCP server assigning domain names based on MAC address. Otherwise, you can leave this field unchanged.</li> <li><code>name</code> - name of server2 (<code>\"name\" = \"services\"</code>)</li> <li><code>hostname</code> - hostname of server2 (<code>\"name\" = \"services.foo.com\"</code>)</li> <li>MAC address (<code>:mac =&gt; \"xxxxxxxx\"</code>). This change is required if you have a DHCP server assigning domain names based on MAC address. Otherwise, you can leave this field unchanged.</li> <li>Other adjustments are optional.</li> </ol>"},{"location":"admin/vagrant/two-machines.html#launch-platform-default-services","title":"Launch platform default services","text":"<p>RabbitMQ, Grafana and InfluxDB services are provisioned on this server.  InfluxDB webUI will be available at: services.foo.com. The RabbitMQ service and its management interface shall be available at 5672 and 15672 TCP ports respectively. The Grafana service shall be available at TCP port 3000.</p> <p>The firewall and network access settings of corporate / cloud network need to be configured to allow external access to the services. Otherwise the users of DTaaS will not be able to utilize these services from their user workspaces.</p> <p>Execute the following commands from terminal to start the machine.</p> <pre><code>vagrant up --provision services\nvagrant ssh services\nwget https://raw.githubusercontent.com/INTO-CPS-Association/DTaaS/release-v0.2/deploy/vagrant/two-machine/services.sh\nbash services.sh\nwget https://raw.githubusercontent.com/INTO-CPS-Association/DTaaS/release-v0.2/deploy/vagrant/route.sh\nsudo bash route.sh\n</code></pre> <p>After the server is up and running, you can see the following services active within server2.</p> service external url Influx visualization service services.foo.com Grafana visualization service services.foo.com:3000 RabbitMQ communication service services.foo.com:5672 RabbitMQ management service services.foo.com:15672"},{"location":"admin/vagrant/two-machines.html#launch-dtaas-application","title":"Launch DTaaS application","text":"<p>Execute the following commands from terminal</p> <pre><code>vagrant up --provision dtaas\nvagrant ssh dtaas\nwget https://raw.githubusercontent.com/INTO-CPS-Association/DTaaS/release-v0.2/deploy/vagrant/route.sh\nsudo bash route.sh\n</code></pre> <p>If you only want to test the application and are not setting up a production instance, you can follow the instructions of single script install.</p> <p>If you are not in a hurry and would rather have a production instance, follow the instructions of regular server installation setup to complete the installation.</p>"},{"location":"user/features.html","title":"Features","text":"<p>Each installation of DTaaS platform comes with the features highlighted in the following picture.</p> <p></p> <p>All the users have dedicated workspaces. These workspaces are dockerized versions of Linux Desktops. The user desktops are isolated so the installations and customizations done in one user workspace do not effect the other user workspaces.</p> <p>Each user workspace comes with some development tools pre-installed. These tools are directly accessible from web browser. The following tools are available at present:</p> Tool Advantage Jupyter Lab Provides flexible creation and use of digital twins and their components from web browser. All the native Jupyterlab usecases are supported here. Jupyter Notebook Useful for web-based management of their files (library assets) VS Code in the browser A popular IDE for software development. Users can develop their digital twin-related assets here. ungit An interactive git client. Users can work with git repositories from web browser <p>In addition, users have access to xfce-based remote desktop via VNC client. The VNC client is available right in the web browser. The xfce supported desktop software can also be run in their workspace.</p> <p>The DTaaS software platform has some pre-installed services available. The currently available services are:</p> Service Advantage InfluxDB time-series database primarly for storing time-series data from physical twins. The digital twins can use an already existing data. Users can also create visualization dashboards for their digital twins. RabbitMQ communication broker for communication between physical and digital twins Grafana Users can create visualization dashboards for their digital twins. <p>In addition, the workspaces are connected to the Internet so all the Digital Twins run within their workspace can interact with both the internal and external services.</p> <p>The users can publish and reuse the digital twin assets available on the platform. In addition, users can run their digital twins and make these live digital twins available as services to their clients. The clients need not be users of the DTaaS software installation.</p>"},{"location":"user/digital-twins/create.html","title":"Create a Digital Twin","text":"<p>The first step in digital twin creation is to use the available assets in your workspace. If you have assets / files in your computer that need to be available in the DTaaS workspace, then please follow the instructions provided in library assets.</p> <p>There are dependencies among the library assets. These dependencies are shown below.</p> <p></p> <p>A digital twin can only be created by linking the assets in a meaningful way. This relationship can be expressed using a mathematical equation:</p> <p> </p> <p>where D denotes data, M denotes models, F denotes functions, T denotes tools,  denotes DT configuration and  is a symbolic notation for a digital twin itself. The   expression denotes composition of DT from D,M,T and F assets. The  indicates zero or one more instances of an asset and  indicates one or more instances of an asset.</p> <p>The DT configuration specifies the relevant assets to use, the potential parameters to be set for these assets. If a DT needs to use RabbitMQ, InfluxDB like services supported by the platform, the DT configuration needs to have access credentials for these services.</p> <p>This kind of generic DT definition is based on the DT examples seen in the wild. You are at liberty to deviate from this definition of DT. The only requirement is the ability to run the DT from either commandline or desktop.</p> <p>Tip</p> <p>If you are stepping into the world of Digital Twins, you might not have distinct digital twin assets. You are likely to have one directory of everything in which you run your digital twin. In such a case we recommend that you upload this monolithic digital twin into digital twin/your_digital_twin_name directory.</p>"},{"location":"user/digital-twins/create.html#example","title":"Example","text":"<p>The Examples repository contains a co-simulation setup for mass spring damper. The complete details on this example are available on github.</p> <p>This example illustrates the potential of using co-simulation for digital twins.</p> <p>The file system contents for this example are:</p> <pre><code>workspace/\n  data/\n    mass-spring-damper\n        input/\n        output/\n\n  digital twins/\n    mass-spring-damper/\n      cosim.json\n      time.json\n      lifecycle/\n        analyze\n        clean\n        evolve\n        execute\n        save\n        terminate\n      README.md\n\n  functions/\n  models/\n    MassSpringDamper1.fmu\n    MassSpringDamper2.fmu\n\n  tools/\n\n  common/\n    data/\n    functions/\n    models/\n    tools/\n        maestro-2.3.0-jar-with-dependencies.jar\n</code></pre> <p>The <code>workspace/data/mass-spring-damper/</code> contains <code>input</code> and <code>output</code> data for the mass-spring-damper digital twin.</p> <p>The two FMU models needed for this digital twin are in <code>models/</code> directory. </p> <p>The co-simulation digital twin needs Maestro co-simulation orchestrator. Since this is a reusable asset for all the co-simulation based DTs, the tool has been placed in <code>common/tools/</code> directory.</p> <p>The actual digital twin configuration is specified in <code>digital twins/mass-spring-damper</code> directory. The co-simulation configuration is specified in two json files, namely <code>cosim.json</code> and <code>time.json</code>. A small explanation of digital twin for its users can be placed in <code>digital twins/mass-spring-damper/README.md</code>.</p> <p>The launch program for this digital twin is in <code>digital twins/mass-spring-damper/lifecycle/execute</code>. This launch program runs the co-simulation digital twin. The co-simulation runs till completion and then ends. The programs in <code>digital twins/mass-spring-damper/lifecycle</code> are responsible for lifecycle management of this digital twin. The lifecycle page provides more explanation on these programs.</p> <p>Execution of a Digital Twin</p> <p>A frequent question arises on the run time characteristics of a digital twin. The natural intuition is to say that a digital twin must operate as long as its physical twin is in operation. If a digital twin runs for a finite time and then ends, can it be called a digital twin?</p> <p>The answer is a resounding YES. The Industry 4.0 usecases seen among SMEs have digital twins that run for a finite time. These digital twins are often run at the discretion of the user.</p> <p>You can run this digital twin by </p> <ol> <li>Go to Workbench tools page of the DTaaS website and open VNC Desktop. This opens a new tab in your browser</li> <li>A page with VNC Desktop and a connect button comes up. Click on Connect. You are now connected to the Linux Desktop of your workspace.</li> <li>Open a Terminal (black rectangular icon in the top left region of your tab) and type the following commands.</li> <li>Download the example files <pre><code>$wget https://github.com/INTO-CPS-Association/DTaaS-examples/archive/refs/heads/main.zip\n$unzip main.zip\n</code></pre></li> <li>Open a file browser and copy the files from this uncompressed folder into your workspace folder (<code>/workspace</code>). Make sure that the file placement matches the one given above.</li> <li>Go to the digital twin directory and run     <pre><code>$cd /workspace/digital twins/mass-spring-damper\n$lifecycle/execute\n</code></pre>     The last command executes the mass-spring-damper digital twin and stores the co-simulation output in <code>data/mass-spring-damper/output</code>.</li> </ol>"},{"location":"user/digital-twins/lifecycle.html","title":"Lifecycle","text":"<p>A DT lifecycle consists of explore, create, execute, save, analyse, evolve and terminate phases.</p> Phase Main Activities explore selection of suitable assets based on the user needs and checking their compatibility for the purposes of creating a DT. create specification of DT configuration. If DT already exists, there is no creation phase at the time of reuse. execute automated / manual execution of a DT based on its configuration. The DT configuration must checked before starting the execution phase. analyse checking the outputs of a DT and making a decision. The outputs can be text files, or visual dashboards. evolve reconfigure DT primarily based on analysis. save involves saving the state of DT to enable future recovery. terminate stop the execution of DT. <p>A complete digital twin will support all the phases but it is not mandatory.</p> <p>Even though not mandatory, having a coding structure makes it easy to manage DT lifecycle phases. It is recommended to have the following structure</p> <pre><code>workspace/\n  digital twins/\n    digital-twin-1/\n      lifecycle/\n        analyze\n        clean\n        evolve\n        execute\n        save\n        terminate\n</code></pre> <p>A dedicated program exists for each phase of DT lifecycle. Each program can be as simple as a script that launches other programs or sends messages to a live digital twin.</p>"},{"location":"user/digital-twins/lifecycle.html#examples","title":"Examples","text":"<p>Here are the programs / scripts to manage three phases in the lifecycle of mass-spring-damper DT.</p> lifecycle/execute<pre><code>#!/bin/bash\nmkdir -p /workspace/data/mass-spring-damper/output\n#cd ..\njava -jar /workspace/common/tools/maestro-2.3.0-jar-with-dependencies.jar \\\nimport -output /workspace/data/mass-spring-damper/output \\\n--dump-intermediate sg1 cosim.json time.json -i -vi FMI2 \\\noutput-dir&gt;debug.log 2&gt;&amp;1\n</code></pre> <p>The execute phases uses the DT configuration, FMU models and Maestro tool to execute the digital twin. The script also stores the output of cosimulation in <code>/workspace/data/mass-spring-damper/output</code>.</p> <p>It is possible for a DT not to support a specific lifecycle phase. This intention can be specified with an empty script and a helpful message if deemed necessary.</p> lifecycle/analyze<pre><code>#!/bin/bash\nprintf \"operation is not supported on this digital twin\"\n</code></pre> <p>The lifecycle programs can call other programs in the code base. In the case of <code>lifecycle/terminate</code> program, it is calling another script to do the necessary job.</p> lifecycle/terminate<pre><code>#!/bin/bash\nlifecycle/clean\n</code></pre>"},{"location":"user/examples/index.html","title":"Index","text":"<p>The examples are hosted in DTaaS examples repository</p>"},{"location":"user/examples/incubator/index.html","title":"Index","text":"<p>Installation of required python packages for the Incubator demo</p> <pre><code>pip install pyhocon\npip install influxdb_client\npip install scipy pip install pandas  pip install pika  pip install oomodelling pip install control  pip install filterpy pip install sympy  pip install docker  </code></pre> <p>start rabbitmq server and create a rabbitmq account with,</p> <pre><code>name: incubator  \npassword:incubator   \nwith access to the virtual host \"/\"\n</code></pre> <pre><code>docker run -d \\\n--name rabbitmq-server \\\n-p 15672:15672 -p 5672:5672 \\\nrabbitmq:3-management\n\ndocker exec rabbitmq-server rabbitmqctl add_user incubator incubator\ndocker exec rabbitmq-server rabbitmqctl set_permissions -p \"/\" incubator \".*\" \".*\" \".*\"\n</code></pre> <p>Access InfluxDB running on another machine. Remember that InfluxDB works only on a distinct sub-domain name like <code>influx.foo.com</code>, but not on <code>foo.com/influx</code>.</p> <pre><code>ssh -i /vagrant/vagrant -fNT -L 40000:localhost:80 vagrant@influx.server2.com\n</code></pre> <p>Update the rabbitmq-server and influxdb configuration in </p> <pre><code>/home/vagrant/dt/1/incubator/example_digital-twin_incubator/software/startup.conf\n</code></pre> <p>select (comment / uncomment) functions in  </p> <pre><code>/home/vagrant/dt/1/incubator/example_digital-twin_incubator/software/startup/start_all_services.py\n</code></pre> <p>Start the program</p> <pre><code>export PYTHONPATH=\"${PYTHONPATH}:/home/vagrant/dt/1/incubator/example_digital-twin_incubator/software/incubator\"\ncd /home/vagrant/dt/1/incubator/example_digital-twin_incubator/software\npython3 -m startup.start_all_services\n</code></pre>"},{"location":"user/servers/lib/LIB-MS.html","title":"Library Microservice","text":"<p> The library microservice provides an API interface to reusable assets library. This is only for expert users who need to integrate the DTaaS with their own IT systems. Regular users can safely skip this page.</p> <p>The lib microservice is responsible for handling and serving the contents of library assets of the DTaaS platform. It provides API endpoints for clients to query, and fetch these assets.</p> <p>This document provides instructions for using the library microservice.</p> <p>Please see assets for a suggested storage conventions of your library assets.</p> <p>Once the assets are stored in the library, you can access the server's endpoint by typing in the following URL: <code>http://foo.com/lib</code>.</p> <p>The URL opens a graphql playground. You can check the query schema and try sample queries here. You can also send graphql queries as HTTP POST requests and get responses.</p>"},{"location":"user/servers/lib/LIB-MS.html#the-graphql-queries","title":"The GraphQL Queries","text":"<p>The library microservice services two graphql requests:</p> <ul> <li>Provide a list of contents for a directory</li> <li>Fetch a file from the available files</li> </ul> <p>The format of the accepted queries are:</p>"},{"location":"user/servers/lib/LIB-MS.html#provide-list-of-contents-for-a-directory","title":"Provide list of contents for a directory","text":"<p>send requests to: https://foo.com/lib</p> GraphQL QueryGraphQL Response <pre><code>query {\n  listDirectory(path: \"user1\") {\n    repository {\n      tree {\n        blobs {\n          edges {\n            node {\n              name\n              type\n            }\n          }\n        }\n        trees {\n          edges {\n            node {\n              name\n              type\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <pre><code>{\n  \"data\": {\n    \"listDirectory\": {\n      \"repository\": {\n        \"tree\": {\n          \"blobs\": {\n            \"edges\": []\n          },\n          \"trees\": {\n            \"edges\": [\n              {\n                \"node\": {\n                  \"name\": \"common\",\n                  \"type\": \"tree\"\n                }\n              },\n              {\n                \"node\": {\n                  \"name\": \"data\",\n                  \"type\": \"tree\"\n                }\n              },\n              {\n                \"node\": {\n                  \"name\": \"digital twins\",\n                  \"type\": \"tree\"\n                }\n              },\n              {\n                \"node\": {\n                  \"name\": \"functions\",\n                  \"type\": \"tree\"\n                }\n              },\n              {\n                \"node\": {\n                  \"name\": \"models\",\n                  \"type\": \"tree\"\n                }\n              },\n              {\n                \"node\": {\n                  \"name\": \"tools\",\n                  \"type\": \"tree\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"user/servers/lib/LIB-MS.html#fetch-a-file-from-the-available-files","title":"Fetch a file from the available files","text":"GraphQL RequestGraphQL Response <pre><code>query {\n  readFile(path: \"user2/data/sample.txt\") {\n    repository {\n      blobs {\n        nodes {\n          name\n          rawBlob\n          rawTextBlob\n        }\n      }\n    }\n  }\n}\n</code></pre> <pre><code>{\n  \"data\": {\n    \"readFile\": {\n      \"repository\": {\n        \"blobs\": {\n          \"nodes\": [\n            {\n              \"name\": \"sample.txt\",\n              \"rawBlob\": \"hello world\",\n              \"rawTextBlob\": \"hello world\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>The path refers to the file path to look at: For example, user1 looks at files of user1; user1/functions looks at contents of functions/ directory.</p>"},{"location":"user/servers/lib/assets.html","title":"Reusable Assets","text":"<p>The reusability of digital twin assets makes it easy for users to work with the digital twins. The reusability of assets is a fundamental feature of the platform.</p>"},{"location":"user/servers/lib/assets.html#kinds-of-reusable-assets","title":"Kinds of Reusable Assets","text":"<p>The DTaaS software categorizes all the reusable library assets into five categories:</p> <p></p>"},{"location":"user/servers/lib/assets.html#functions","title":"Functions","text":"<p>The functions responsible for pre- and post-processing of: data inputs, data outputs, control outputs. The data science libraries and functions can be used to create useful function assets for the platform. In some cases, Digital Twin models require calibration prior to their use; functions written by domain experts along with right data inputs can make model calibration an achievable goal. Another use of functions is to process the sensor and actuator data of both Physical Twins and Digital Twins.</p>"},{"location":"user/servers/lib/assets.html#data","title":"Data","text":"<p>The data sources and sinks available to a digital twins. Typical examples of data sources are sensor measurements from  Physical Twins, and test data provided by manufacturers for calibration of models. Typical examples of data sinks are visualization software, external users and data storage services. There exist special outputs such as events, and commands which are akin to control outputs from a Digital Twin. These control outputs usually go to Physical Twins, but they can also go to another Digital Twin.</p>"},{"location":"user/servers/lib/assets.html#models","title":"Models","text":"<p>The model assets are used to describe different aspects of Physical Twins and their environment, at different levels of abstraction. Therefore, it is possible to have multiple models for the same Physical Twin. For example, a flexible robot used in a car production plant may have structural model(s) which will be useful in tracking the wear and tear of parts. The same robot can have a behavioural model(s) describing the safety guarantees provided by the robot manufacturer. The same robot can also have a functional model(s) describing the part manufacturing capabilities of the robot.</p>"},{"location":"user/servers/lib/assets.html#tools","title":"Tools","text":"<p>The software tool assets are software used to create, evaluate and analyze models. These tools are executed on top of a computing platforms, i.e., an operating system, or virtual machines like Java virtual machine, or inside docker containers. The tools tend to be platform specific, making them less reusable than models.  A tool can be packaged to run on a local or distributed virtual machine environments thus allowing selection of most suitable execution environment for a Digital Twin.  Most models require tools to evaluate them in the context of data inputs.  There exist cases where executable packages are run as binaries in a computing environment. Each of these packages are a pre-packaged combination of models and tools put together to create a ready to use Digital Twins.</p>"},{"location":"user/servers/lib/assets.html#digital-twins","title":"Digital Twins","text":"<p>These are ready to use digital twins created by one or more users. These digital twins can be reconfigured later for specific use cases.</p>"},{"location":"user/servers/lib/assets.html#file-system-structure","title":"File System Structure","text":"<p>Each user has their assets put into five different directories named above. In addition, there will also be common library assets that all users have access to. A simplified example of the structure is as follows:</p> <pre><code>workspace/\n  data/\n    data1/ (ex: sensor)\n      filename (ex: sensor.csv)\n      README.md\n    data2/ (ex: turbine)\n      README.md (remote source; no local file)\n    ...\n  digital twins/\n    digital twin-1/ (ex: incubator)\n      code and config\n      README.md (usage instructions)\n    digital twin-2/ (ex: mass spring damper)\n      code and config\n      README.md (usage instructions)\n    digital twin-3/ (ex: model swap)\n      code and config\n      README.md (usage instructions)\n    ...\n  functions/\n    function1/ (ex: graphs)\n      filename (ex: graphs.py)\n      README.md\n    function2/ (ex: statistics)\n      filename (ex: statistics.py)\n      README.md\n    ...\n  models/\n    model1/ (ex: spring)\n      filename (ex: spring.fmu)\n      README.md\n    model2/ (ex: building)\n      filename (ex: building.skp)\n      README.md\n    model3/ (ex: rabbitmq)\n      filename (ex: rabbitmq.fmu)\n      README.md\n    ...\n  tools/\n    tool1/ (ex: maestro)\n      filename (ex: maestro.jar)\n      README.md\n    ...\n  common/\n    data/\n    functions/\n    models/\n    tools/\n</code></pre> <p>Tip</p> <p>The DTaaS is agnostic to the format of your assets. The only requirement is that they are files which can be uploaded on the Library page. Any directories can be compressed as one file and uploaded. You can decompress the file into a directory from a Terminal or xfce Desktop available on the Workbench page.</p> <p>A recommended file system structure for storing assets is also available in DTaaS examples.</p>"},{"location":"user/servers/lib/assets.html#create-assets","title":"Create Assets","text":"<p>The DTaaS software allows users to create new library assets on the platform.</p> <p></p> <p>Users can install asset authoring tools in their own workspace. These authoring tools can then be used to create and publish new assets. User workspaces are private and are not shared with other users. Thus any licensed software tools installed in their workspace is only available to them.</p>"},{"location":"user/servers/lib/assets.html#upload-assets","title":"Upload Assets","text":"<p>Users can upload assets into their workspace using Library page of the website.</p> <p></p> <p>You can go into a directory and click on the upload button to upload a file or a directory into your workspace. This asset is then available in all the workbench tools you can use. You can also create new assets on the page by clicking on new drop down menu. This is a simple web interface which allows you to create text-based files. You need to upload other files using upload button.</p>"},{"location":"user/website/index.html","title":"Website","text":"<p>This page contains a screenshot driven preview of the website.</p>"},{"location":"user/website/index.html#login-to-enter-the-dtaas-software-platform","title":"Login to enter the DTaaS software platform","text":"<p>The screen presents with HTTP authentication form. You can enter the user credentials. You will be using HTTPS secure communication so the username and password are secure.</p>"},{"location":"user/website/index.html#enter-username-again","title":"Enter username again","text":"<p>You are now logged into the server. You can enter the same username again to log into your workspace.</p>"},{"location":"user/website/index.html#overview-of-menu-items","title":"Overview of menu items","text":"<p>The menu is hidden by default. Only the icons of menu items are visible. You can click on the  icon in the top-left corner of the page to see the menu.</p> <p></p> <p>There are three menu items:</p> <p>Library: for management of reusable library assets. You can upload, download, create and modify new files on this page.</p> <p>Digital Twins: for management of digital twins. You are presented with Jupyter Lab page from which you can run the digital twins.</p> <p>Workbench: Not all digital twins can be managed within Jupyter Lab. You have more tools at your disposal on this page.</p>"},{"location":"user/website/index.html#library-tabs-and-their-help-text","title":"Library tabs and their help text","text":"<p>You can see the file manager and five tabs above the library manager. Each tab provides help text to guide users in the use of different directories in their workspace.</p> Functions <p>The functions responsible for pre- and post-processing of: data inputs, data outputs, control outputs. The data science libraries and functions can be used to create useful function assets for the platform.</p> <p>In some cases, Digital Twin models require calibration prior to their use; functions written by domain experts along with right data inputs can make model calibration an achievable goal. Another use of functions is to process the sensor and actuator data of both Physical Twins and Digital Twins. ```</p> Data <p>The data sources and sinks available to a digital twins. Typical examples of data sources are sensor measurements from  Physical Twins, and test data provided by manufacturers for calibration of models. Typical examples of data sinks are visualization software, external users and data storage services. There exist special outputs such as events, and commands which are akin to control outputs from a Digital Twin. These control outputs usually go to Physical Twins, but they can also go to another Digital Twin.</p> Models <p>The model assets are used to describe different aspects of Physical Twins and their environment, at different levels of abstraction. Therefore, it is possible to have multiple models for the same Physical Twin. For example, a flexible robot used in a car production plant may have structural model(s) which will be useful in tracking the wear and tear of parts. The same robot can have a behavioural model(s) describing the safety guarantees provided by the robot manufacturer. The same robot can also have a functional model(s) describing the part manufacturing capabilities of the robot.</p> Tools <p>The software tool assets are software used to create, evaluate and analyze models. These tools are executed on top of a computing platforms, i.e., an operating system, or virtual machines like Java virtual machine, or inside docker containers. The tools tend to be platform specific, making them less reusable than models. </p> <p>A tool can be packaged to run on a local or distributed virtual machine environments thus allowing selection of most suitable execution environment for a Digital Twin. </p> <p>Most models require tools to evaluate them in the context of data inputs.  There exist cases where executable packages are run as binaries in a computing environment. Each of these packages are a pre-packaged combination of models and tools put together to create a ready to use Digital Twins.</p> Digital <p>These are ready to use digital twins created by one or more users. These digital twins can be reconfigured later for specific use cases.</p> <p>In addition to the five directories, there is also common directory in which five sub-directories exist. These sub-directories are: data, functions, models, tools and digital twins.</p> Common <p>The common directory again has four sub-directories:</p> <ul> <li>data</li> <li>functions</li> <li>models</li> <li>tools</li> <li>digital twins</li> </ul> <p>The assets common to all users are placed in common.</p> <p>The items used by more than one user are placed in common. The items in the common directory are available to all users. Further explanation of directory structure and placement of reusable assets within the the directory structure is in the assets page</p> <p> The file manager is based on Jupyter Notebook and all the tasks you can perform in the Jupyter Notebook can be undertaken here.</p>"},{"location":"user/website/index.html#digital-twins-page","title":"Digital Twins page","text":"<p>The digital twins page has three tabs and the central pane opens Jupyter Lab. There are three tabs with helpful instructions on the suggested tasks you can undertake in the Create - Execute - Analyze life cycle phases of digital twin. You can see more explanation on the life cycle phases of digital twin.</p> Create <p>Create digital twins from tools provided within user workspaces. Each digital twin will have one directory. It is suggested that user provide one bash shell script to run their digital twin. Users can create the required scripts and other files from tools provided in Workbench page.</p> Execute <p>Digital twins are executed from within user workspaces. The given bash script gets executed from digital twin directory. Terminal-based digital twins can be executed from VSCode and graphical digital twins can be executed from VNC GUI. The results of execution can be placed in the data directory.</p> Analyze <p>The analysis of digital twins requires running of digital twin script from user workspace. The execution results placed within data directory are processed by analysis scripts and results are placed back in the data directory. These scripts can either be executed from VSCode and graphical results or can be executed from VNC GUI.</p> <p> The reusable assets (files) seen in the file manager are available in the Jupyter Lab. In addition, there is a git plugin installed in the Jupyter Lab using which you can link your files with the external git repositories.</p>"},{"location":"user/website/index.html#workbench","title":"Workbench","text":"<p>The workbench page provides links to four integrated tools.</p> <p></p> <p>The hyperlinks open in new browser tab. The screenshots of pages opened in new browser are:</p> <p></p> <p>Bug</p> <p>The Terminal hyperlink does not always work reliably. If you want terminal. Please use the tools dropdown in the Jupyter Notebook.</p>"},{"location":"user/website/index.html#finally-logout","title":"Finally logout","text":"<p>You have to close the browser in order to completely exit the DTaaS software platform.</p>"}]}